## Day 3 â€” Test Runner & Structured Failure Detection

**Date:** February 23, 2026  
**Duration:** 2 hours  
**Status:** âœ… Complete

### Goals

Transform vague â€œit crashedâ€ feedback into precise â€œtest_factorial_of_5 failed: expected 120, got 206â€.

### The Problem

**Day 1-2 behavior:**
Agent writes code, runs it, sees output. Success/failure is binary:

- Did it crash? â†’ Failure
- Did it run? â†’ Success

**Example:**

```python
def factorial(n):
    return n * factorial(n - 1)  # Missing base case

print(factorial(5))
```

Agent sees: `RecursionError: maximum recursion depth exceeded`  
But it doesnâ€™t know:

- Which specific test case failed?
- What was expected vs actual?
- Did edge cases work?

**What we needed:**

- âœ… Write test cases alongside code
- âœ… Run tests automatically
- âœ… Get structured pass/fail per test
- âœ… See expected vs actual values
- âœ… Know which tests passed/failed

### The Solution: unittest Integration

Pythonâ€™s built-in `unittest` framework provides:

- Test case classes (`unittest.TestCase`)
- Assertions (`assertEqual`, `assertRaises`, etc.)
- Test discovery and running
- Structured results

### Implementation

#### 1. Test Runner (`src/test_runner.py`)

New file: 255 lines

**Key function: `run_tests(code)`**

**What it does:**

1. Executes code to define test classes
1. Finds all classes inheriting from `unittest.TestCase`
1. Loads tests from those classes
1. Runs tests with `unittest.TextTestRunner`
1. Parses failures and errors
1. Returns structured `TestResult` dataclass

**TestResult dataclass:**

```python
@dataclass
class TestResult:
    total_tests: int
    passed: int
    failed: int
    errors: int
    failures: List[dict]  # [{test_name, error_type, message, traceback}, ...]
    success: bool  # True only if all tests passed
```

**Failure dict structure:**

```python
{
    'test_name': 'test_factorial_of_5 (TestFactorial)',
    'error_type': 'AssertionError',
    'message': 'AssertionError: 206 != 120',
    'traceback': 'Traceback (most recent call last):\n  ...'
}
```

**Edge cases handled:**

- No test class found â†’ error with message â€œNo unittest.TestCase class foundâ€
- Code fails to execute â†’ error with exception details
- Multiple test classes â†’ runs all of them

#### 2. Updated Agent Prompt (`src/agent.py`)

**Old system prompt:**

```python
"Write complete, runnable code â€” no placeholders or TODOs"
```

**New system prompt:**

```python
"""Always write code WITH unit tests using Python's unittest framework

Example structure:
```python
import unittest

# Your main code
def my_function(x):
    return x * 2

# Your tests
class TestMyFunction(unittest.TestCase):
    def test_positive_number(self):
        self.assertEqual(my_function(5), 10)
    
    def test_zero(self):
        self.assertEqual(my_function(0), 0)
```

â€œâ€â€

```
**Impact:** Claude now always writes tests alongside code, covering normal cases, edge cases, and error cases.

#### 3. Updated Executor (`src/executor.py`)

**Added test detection:**
```python
code_has_tests = 'unittest.TestCase' in code
```

**If tests detected and code runs successfully:**

```python
test_result = run_tests(code)

if test_result.success:
    return ExecutionResult(
        success=True,
        output=f"âœ… All tests passed! ({test_result.passed}/{test_result.total_tests})"
    )
else:
    # Build detailed failure message
    failure_details = []
    for failure in test_result.failures:
        failure_details.append(
            f"âŒ {failure['test_name']}\n"
            f"   {failure['message']}"
        )
    
    error_msg = (
        f"Tests failed: {test_result.passed}/{test_result.total_tests} passed\n\n"
        + "\n\n".join(failure_details)
    )
    
    return ExecutionResult(success=False, error=error_msg)
```

**If no tests detected:**
Falls back to Day 1-2 behavior (check exit code only).

### Testing Results

#### Test 1: Passing Tests

```python
import unittest

def factorial(n):
    if n <= 1:
        return 1
    return n * factorial(n - 1)

class TestFactorial(unittest.TestCase):
    def test_five(self):
        self.assertEqual(factorial(5), 120)
    
    def test_zero(self):
        self.assertEqual(factorial(0), 1)
```

**Result:**

```
ExecutionResult(
    success=True,
    output='âœ… All tests passed! (2/2)',
    error='',
    execution_time=0.32s
)
```

#### Test 2: Failing Tests

```python
import unittest

def factorial(n):
    if n <= 1:
        return 1
    return n * factorial(n - 1) + 1  # BUG: added +1

class TestFactorial(unittest.TestCase):
    def test_five(self):
        self.assertEqual(factorial(5), 120)
    
    def test_zero(self):
        self.assertEqual(factorial(0), 1)
```

**Result:**

```
ExecutionResult(
    success=False,
    error='''Tests failed: 0/2 passed

âŒ test_five (builtins.TestFactorial)
   AssertionError: 206 != 120

âŒ test_zero (builtins.TestFactorial)
   AssertionError: 2 != 1''',
    execution_time=0.35s
)
```

**Analysis:**

- Agent sees exactly which tests failed
- Expected: 120, Got: 206 (5! + 5 from the +1 bug)
- Expected: 1, Got: 2 (0! + 1 from the +1 bug)
- Agent can now fix the specific bug (remove the +1)

#### Test 3: Mixed Results

```python
import unittest

def factorial(n):
    if n < 0:
        raise ValueError("Negative not allowed")
    if n <= 1:
        return 1
    return n * factorial(n - 1)

class TestFactorial(unittest.TestCase):
    def test_five(self):
        self.assertEqual(factorial(5), 120)  # PASS
    
    def test_negative(self):
        with self.assertRaises(ValueError):
            factorial(-1)  # PASS
    
    def test_large(self):
        self.assertEqual(factorial(10), 3628800)  # PASS
    
    def test_two(self):
        self.assertEqual(factorial(2), 3)  # FAIL: 2! = 2, not 3
```

**Result:**

```
Tests failed: 3/4 passed

âŒ test_two (builtins.TestFactorial)
   AssertionError: 2 != 3
```

Agent can see that 3 out of 4 tests passed, and only `test_two` failed with a wrong expectation.

### Files Created/Modified

**New file:**

- `src/test_runner.py` â€” 135 lines

**Modified files:**

- `src/agent.py` â€” Added import, rewrote `build_system_prompt()` (+30 lines)
- `src/executor.py` â€” Added test detection and running (+40 lines)

**Total new code:** ~205 lines

### Concepts Learned

**unittest framework:**  
Pythonâ€™s built-in testing framework. Test classes inherit from `unittest.TestCase`. Methods starting with `test_` are run automatically.

**Assertions:**

- `assertEqual(a, b)` â€” checks a == b
- `assertTrue(x)` â€” checks x is truthy
- `assertRaises(Exception)` â€” checks exception is raised
- Many more: `assertIn`, `assertIsNone`, `assertGreater`, etc.

**Test discovery:**  
`unittest.TestLoader().loadTestsFromTestCase(TestClass)` finds all test methods in a class.

**Test execution:**  
`unittest.TextTestRunner().run(suite)` runs tests and returns a result object with `.failures` and `.errors`.

**Failures vs Errors:**

- **Failure:** Assertion failed (expected != actual)
- **Error:** Exception raised during test (code crashed)

**exec() for dynamic execution:**  
`exec(code, namespace)` executes code and populates namespace with defined classes/functions. We use this to extract test classes.

**Structured feedback:**  
Instead of raw exception strings, we parse and structure failures into dicts with test name, error type, message, and traceback. This enables the agent to understand what went wrong.

### Why This Matters

**Before Day 3:**

```
Agent: I wrote a factorial function
Executor: RecursionError on line 4
Agent: I'll add a base case
```

Vague, slow iteration.

**After Day 3:**

```
Agent: I wrote factorial with 4 tests
Executor: âŒ test_factorial_of_5 failed: expected 120, got 206
         âŒ test_factorial_of_0 failed: expected 1, got 2
         âœ… test_negative_raises passed
         âœ… test_one_returns_one passed
Agent: The bug is +1 in the recursive call, I'll remove it
```

Precise, fast iteration.

### Commands Used

```bash
# Create test runner
touch src/test_runner.py
open -e src/test_runner.py

# Test the test runner
touch test_runner_demo.py
python3 -c "from src.test_runner import run_tests; result = run_tests(open('test_runner_demo.py').read()); print(result)"

# Update agent
open -e src/agent.py
# (Added import, rewrote system prompt)
python3 -c "from src.agent import build_system_prompt; print(build_system_prompt()[:100])"

# Update executor
open -e src/executor.py
# (Added import, added test detection logic)
python3 -c "from src.executor import execute_code; print('Import successful')"

# Test integration with passing tests
python3 -c "from src.executor import execute_code; code = '''...(passing test code)...'''; result = execute_code(code); print(result)"

# Test integration with failing tests
python3 -c "from src.executor import execute_code; code = '''...(failing test code)...'''; result = execute_code(code); print(result)"

# Clean up
rm test_runner_demo.py

# Add screenshots
mkdir screenshots
# (Manually added passing_test.png and failing_test.png)

# Commit
git add .
git commit -m "Day 3: test runner with unittest integration and structured failure detection"
git push
```

### Screenshots Added

- `screenshots/passing_test.png` â€” Shows â€œâœ… All tests passed! (2/2)â€
- `screenshots/failing_test.png` â€” Shows detailed failure messages with expected vs actual

### Next Steps (Day 4)

- Add short-term memory (last 5 attempts)
- Implement reflection mechanism
- Agent analyzes why it failed before next attempt
- Avoid repeating the same mistake twice

### Improvement Over Days 1-2

|Aspect       |Day 1-2                |Day 3                                       |
|-------------|-----------------------|--------------------------------------------|
|Feedback     |â€œRecursionErrorâ€       |â€œtest_factorial_of_5: expected 120, got 206â€|
|Precision    |Binary (crashed or ran)|Per-test pass/fail                          |
|Actionability|Guess whatâ€™s wrong     |Know exactly whatâ€™s wrong                   |
|Edge cases   |Untested               |Multiple test cases cover them              |
|Confidence   |â€œI think it worksâ€     |â€œAll 4 tests passedâ€                        |

**Result:** Agent can now self-correct with surgical precision instead of trial-and-error guessing.

-----

## Summary Stats

### Overall Progress

- **Days completed:** 3 / 7
- **Total development time:** ~7 hours
- **Total lines of code:** ~1,094 lines (excluding tests and docs)
- **Files created:** 15
- **Bugs fixed:** 2
- **Git commits:** 3

### Files by Size

1. `src/agent.py` â€” 178 lines
1. `src/executor.py` â€” 255 lines
1. `src/test_runner.py` â€” 135 lines
1. `main.py` â€” 37 lines
1. `src/config.py` â€” 26 lines
1. `README.md` â€” 489 lines
1. `DAILY_LOG.md` â€” 954 lines (this file)

### Key Achievements

âœ… Production-grade sandbox (Day 2)  
âœ… CPU/memory/file limits (Day 2)  
âœ… Test-driven development (Day 3)  
âœ… Structured failure feedback (Day 3)  
âœ… Self-correcting agent loop (Day 1-3)

### Remaining Work

ğŸ”² Day 4: Short-term memory + reflection  
ğŸ”² Day 5: Long-term memory with vectors  
ğŸ”² Day 6: Circuit breaker + advanced safety  
ğŸ”² Day 7: Polish + CLI improvements + demo

-----

*Last updated: February 23, 2026*
